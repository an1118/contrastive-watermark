{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(dataset, min_length, data_size=500):\n",
    "    data = []\n",
    "    for text in dataset['train']['text']:\n",
    "        text0 = text.split()[0:min_length]\n",
    "        if len(text0) >= min_length:\n",
    "            text0 = ' '.join(text0)\n",
    "            data.append({'text0': text0, 'text': text})\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        if len(data) ==  data_size:\n",
    "            break\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"https://huggingface.co/datasets/allenai/c4/resolve/1ddc917116b730e1859edef32896ec5c16be51d0/realnewslike/c4-train.00000-of-00512.json.gz\"\n",
    "dataset = load_dataset('json', data_files=data)\n",
    "dataset = pre_process(dataset, min_length=200, data_size=100)   # [{text0: 'text0', text: 'text'}]\n",
    "\n",
    "num_sent = []\n",
    "\n",
    "for item in dataset:\n",
    "    text = item['text']\n",
    "    text_sent = nltk.sent_tokenize(text)\n",
    "    num_sent.append(len(text_sent))\n",
    "\n",
    "df = pd.Series(num_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    100.000000\n",
       "mean      26.680000\n",
       "std       16.866008\n",
       "min        7.000000\n",
       "25%       13.750000\n",
       "50%       23.000000\n",
       "75%       33.000000\n",
       "max       85.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'/mnt/data2/lian/projects/watermark/adaptive-text-watermark-yepeng/outputs/watermark-8b-2step.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "df['num_of_sent'] = df['unwatermarked_text'].apply(lambda t: len(nltk.sent_tokenize(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    185.000000\n",
       "mean       9.843243\n",
       "std        2.338773\n",
       "min        1.000000\n",
       "25%        9.000000\n",
       "50%       10.000000\n",
       "75%       11.000000\n",
       "max       15.000000\n",
       "Name: num_of_sent, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['num_of_sent'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get average word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "watermark-8b-1step-10sent.csv\n",
      "227.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_dir = r'/mnt/data2/lian/projects/watermark/adaptive-text-watermark-yepeng/outputs/continue'\n",
    "\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith('csv'):\n",
    "        print(filename)\n",
    "        df = pd.read_csv(os.path.join(data_dir, filename))\n",
    "        df['wm_avg_len'] = df['adaptive_watermarked_text'].apply(lambda x: len(x.strip().split()))\n",
    "        avg_len = df['wm_avg_len'].mean()\n",
    "        print(round(avg_len, 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def word_level_edit_distance(text1, text2):\n",
    "    if not isinstance(text1, str) or not isinstance(text2, str):\n",
    "        return None\n",
    "    \n",
    "    # 将文本分割成单词\n",
    "    words1 = text1.split()\n",
    "    words2 = text2.split()\n",
    "    \n",
    "    # 初始化动态规划矩阵\n",
    "    len1 = len(words1)\n",
    "    len2 = len(words2)\n",
    "    dp = np.zeros((len1 + 1, len2 + 1), dtype=int)\n",
    "    \n",
    "    # 填充第一列和第一行\n",
    "    for i in range(len1 + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len2 + 1):\n",
    "        dp[0][j] = j\n",
    "    \n",
    "    # 填充动态规划矩阵\n",
    "    for i in range(1, len1 + 1):\n",
    "        for j in range(1, len2 + 1):\n",
    "            if words1[i - 1] == words2[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1]  # 如果单词相同，不需要操作\n",
    "            else:\n",
    "                dp[i][j] = min(\n",
    "                    dp[i - 1][j] + 1,   # 删除\n",
    "                    dp[i][j - 1] + 1,   # 插入\n",
    "                    dp[i - 1][j - 1] + 1  # 替换\n",
    "                )\n",
    "    \n",
    "    return dp[len1][len2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = r'/blue/buyuheng/li_an.ucsb/projects/baselines/adaptive-text-watermark/outputs/wm-model-Llama-3.1-8B-Instruct/wm-c4-alpha2.0-delta0.2|0.5.csv'\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:04<00:00, 40.95it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 145.15it/s]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "# Wrap the apply function with tqdm for progress tracking\n",
    "tqdm.pandas()\n",
    "\n",
    "# Compute edit distances for spoofing columns\n",
    "sentiment_spoofing_columns = ['spoofing_watermarked_text', 'latter_sentiment_spoof']\n",
    "for col in sentiment_spoofing_columns:\n",
    "    df[f'edit_distance_{col}'] = df.progress_apply(lambda x: word_level_edit_distance(x['adaptive_watermarked_text'], x[col]), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
