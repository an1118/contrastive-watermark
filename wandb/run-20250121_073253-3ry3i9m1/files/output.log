  0%|                                                                                                                                                                                                                                           | 0/4 [00:00<?, ?it/s][WARNING|logging.py:313] 2025-01-21 07:32:53,722 >> You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:04<00:00,  1.10s/it][INFO|trainer.py:2643] 2025-01-21 07:32:58,402 >>
{'loss': 1.5815, 'grad_norm': nan, 'learning_rate': 3e-05, 'loss_1': 1.5733363628387451, 'loss_2': 0.0081634521484375, 'loss_3': -9.328385353088379, 'loss_4': 8.89427375793457, 'epoch': 0.25}
{'loss': 2.0609, 'grad_norm': nan, 'learning_rate': 3e-05, 'loss_1': 2.020209550857544, 'loss_2': 0.04071044921875, 'loss_3': -10.609344482421875, 'loss_4': 10.623645782470703, 'epoch': 0.5}
{'loss': 0.8035, 'grad_norm': nan, 'learning_rate': 3e-05, 'loss_1': 0.7886104583740234, 'loss_2': 0.01488494873046875, 'loss_3': -11.12866497039795, 'loss_4': 10.601709365844727, 'epoch': 0.75}
{'loss': 1.078, 'grad_norm': nan, 'learning_rate': 3e-05, 'loss_1': 1.0646305084228516, 'loss_2': 0.0134124755859375, 'loss_3': -13.418230056762695, 'loss_4': 13.514850616455078, 'epoch': 1.0}

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:04<00:00,  1.17s/it]
{'train_runtime': 5.6022, 'train_samples_per_second': 5.712, 'train_steps_per_second': 0.714, 'train_loss': 1.3809895515441895, 'epoch': 1.0}
[INFO|trainer.py:3910] 2025-01-21 07:32:58,404 >> Saving model checkpoint to result/testt/end2end-simcse-gte-Qwen2-1.5B-instruct-c4-loss_cl2_gr-wneg1
[INFO|configuration_utils.py:420] 2025-01-21 07:32:58,405 >> Configuration saved in result/testt/end2end-simcse-gte-Qwen2-1.5B-instruct-c4-loss_cl2_gr-wneg1/config.json
[INFO|modeling_utils.py:2996] 2025-01-21 07:33:07,891 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at result/testt/end2end-simcse-gte-Qwen2-1.5B-instruct-c4-loss_cl2_gr-wneg1/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2491] 2025-01-21 07:33:07,892 >> tokenizer config file saved in result/testt/end2end-simcse-gte-Qwen2-1.5B-instruct-c4-loss_cl2_gr-wneg1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-01-21 07:33:07,892 >> Special tokens file saved in result/testt/end2end-simcse-gte-Qwen2-1.5B-instruct-c4-loss_cl2_gr-wneg1/special_tokens_map.json
01/21/2025 07:33:08 - INFO - __main__ -   ***** Train results *****
01/21/2025 07:33:08 - INFO - __main__ -     epoch = 1.0
01/21/2025 07:33:08 - INFO - __main__ -     total_flos = 241800165457920.0
01/21/2025 07:33:08 - INFO - __main__ -     train_loss = 1.3809895515441895
01/21/2025 07:33:08 - INFO - __main__ -     train_runtime = 5.6022
01/21/2025 07:33:08 - INFO - __main__ -     train_samples_per_second = 5.712
01/21/2025 07:33:08 - INFO - __main__ -     train_steps_per_second = 0.714
01/21/2025 07:33:08 - INFO - __main__ -   *** Evaluate ***
[INFO|trainer.py:4226] 2025-01-21 07:33:08,063 >>
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-01-21 07:33:08,063 >>   Num examples = 8
[INFO|trainer.py:4231] 2025-01-21 07:33:08,063 >>   Batch size = 8
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1127.80it/s]
01/21/2025 07:33:08 - INFO - __main__ -   ***** Eval results *****
01/21/2025 07:33:08 - INFO - __main__ -     epoch = 1.0
01/21/2025 07:33:08 - INFO - __main__ -     eval_loss = 2.0169429779052734
01/21/2025 07:33:08 - INFO - __main__ -     eval_loss_1 = 2.0044918060302734
01/21/2025 07:33:08 - INFO - __main__ -     eval_loss_2 = 0.012451171875
01/21/2025 07:33:08 - INFO - __main__ -     eval_loss_3 = -6.817693710327148
01/21/2025 07:33:08 - INFO - __main__ -     eval_loss_4 = 7.042447090148926
01/21/2025 07:33:08 - INFO - __main__ -     eval_runtime = 0.3187
01/21/2025 07:33:08 - INFO - __main__ -     eval_samples_per_second = 25.103
01/21/2025 07:33:08 - INFO - __main__ -     eval_steps_per_second = 3.138
