  0%|                                                                                                                                                                                                                                        | 0/5160 [00:00<?, ?it/s][WARNING|logging.py:313] 2025-01-21 09:18:59,258 >> You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|▏                                                                                                                                                                                                                             | 5/5160 [00:04<1:05:16,  1.32it/s][INFO|trainer.py:4226] 2025-01-21 09:19:03,393 >>
{'loss': 2.7322, 'grad_norm': nan, 'learning_rate': 3e-05, 'loss_1': 2.6553902626037598, 'loss_2': 0.07684326171875, 'loss_3': -13.70328140258789, 'loss_4': 9.949840545654297, 'epoch': 0.01}
{'loss': 3.0957, 'grad_norm': nan, 'learning_rate': 3e-05, 'loss_1': 3.012371301651001, 'loss_2': 0.08331298828125, 'loss_3': -13.331350326538086, 'loss_4': 9.92608642578125, 'epoch': 0.01}
{'loss': 3.0858, 'grad_norm': inf, 'learning_rate': 3e-05, 'loss_1': 3.016918897628784, 'loss_2': 0.06884765625, 'loss_3': -13.476580619812012, 'loss_4': 9.631412506103516, 'epoch': 0.02}
{'loss': 2.6849, 'grad_norm': 138.1249542236328, 'learning_rate': 2.999418604651163e-05, 'loss_1': 2.6182749271392822, 'loss_2': 0.066650390625, 'loss_3': -13.746009826660156, 'loss_4': 9.592080116271973, 'epoch': 0.02}
{'loss': 3.0035, 'grad_norm': 137.05751037597656, 'learning_rate': 2.9988372093023255e-05, 'loss_1': 2.9346344470977783, 'loss_2': 0.06890869140625, 'loss_3': -13.665800094604492, 'loss_4': 9.646661758422852, 'epoch': 0.03}
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-01-21 09:19:03,393 >>   Num examples = 1024
[INFO|trainer.py:4231] 2025-01-21 09:19:03,393 >>   Batch size = 64
  0%|▏                                                                                                                                                                                                                             | 5/5160 [00:07<1:05:16,  1.32it/s][INFO|trainer.py:3910] 2025-01-21 09:19:07,183 >> Saving model checkpoint to SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-5
[INFO|configuration_utils.py:420] 2025-01-21 09:19:07,185 >> Configuration saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-5/config.json                                                                                
{'eval_loss': 1.2392643690109253, 'eval_runtime': 3.7889, 'eval_samples_per_second': 270.261, 'eval_steps_per_second': 4.223, 'eval_loss_1': 1.1843136548995972, 'eval_loss_2': 0.054950714111328125, 'eval_loss_3': -17.85811424255371, 'eval_loss_4': 9.35734748840332, 'epoch': 0.03}
[INFO|modeling_utils.py:2988] 2025-01-21 09:19:07,637 >> Model weights saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-5/model.safetensors
[INFO|tokenization_utils_base.py:2491] 2025-01-21 09:19:07,639 >> tokenizer config file saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-01-21 09:19:07,639 >> Special tokens file saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-5/special_tokens_map.json
  0%|▍                                                                                                                                                                                                                            | 10/5160 [00:12<1:34:31,  1.10s/it][INFO|trainer.py:4226] 2025-01-21 09:19:12,107 >>
{'loss': 2.6577, 'grad_norm': 130.33444213867188, 'learning_rate': 2.9982558139534887e-05, 'loss_1': 2.6005873680114746, 'loss_2': 0.05706787109375, 'loss_3': -14.10137939453125, 'loss_4': 9.600650787353516, 'epoch': 0.03}
{'loss': 2.2335, 'grad_norm': 118.03402709960938, 'learning_rate': 2.9976744186046512e-05, 'loss_1': 2.181689977645874, 'loss_2': 0.051788330078125, 'loss_3': -14.629755973815918, 'loss_4': 10.14033031463623, 'epoch': 0.04}
{'loss': 2.2233, 'grad_norm': 129.17538452148438, 'learning_rate': 2.997093023255814e-05, 'loss_1': 2.1769745349884033, 'loss_2': 0.04632568359375, 'loss_3': -14.784768104553223, 'loss_4': 9.601834297180176, 'epoch': 0.05}
{'loss': 2.122, 'grad_norm': 131.80044555664062, 'learning_rate': 2.996511627906977e-05, 'loss_1': 2.0825486183166504, 'loss_2': 0.03948974609375, 'loss_3': -15.052807807922363, 'loss_4': 10.372245788574219, 'epoch': 0.05}
{'loss': 2.0871, 'grad_norm': 123.208984375, 'learning_rate': 2.9959302325581394e-05, 'loss_1': 2.0631656646728516, 'loss_2': 0.02392578125, 'loss_3': -14.82418441772461, 'loss_4': 10.437522888183594, 'epoch': 0.06}
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-01-21 09:19:12,107 >>   Num examples = 1024
[INFO|trainer.py:4231] 2025-01-21 09:19:12,107 >>   Batch size = 64
  0%|▍                                                                                                                                                                                                                            | 10/5160 [00:16<1:34:31,  1.10s/it][INFO|trainer.py:3910] 2025-01-21 09:19:15,946 >> Saving model checkpoint to SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-10
[INFO|configuration_utils.py:420] 2025-01-21 09:19:15,948 >> Configuration saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-10/config.json                                                                               
{'eval_loss': 0.8288468718528748, 'eval_runtime': 3.8376, 'eval_samples_per_second': 266.833, 'eval_steps_per_second': 4.169, 'eval_loss_1': 0.8176879286766052, 'eval_loss_2': 0.011158943176269531, 'eval_loss_3': -18.167327880859375, 'eval_loss_4': 10.509654998779297, 'epoch': 0.06}
[INFO|modeling_utils.py:2988] 2025-01-21 09:19:16,422 >> Model weights saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-10/model.safetensors
[INFO|tokenization_utils_base.py:2491] 2025-01-21 09:19:16,424 >> tokenizer config file saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-10/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-01-21 09:19:16,424 >> Special tokens file saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-10/special_tokens_map.json
[INFO|trainer.py:4002] 2025-01-21 09:19:17,349 >> Deleting older checkpoint [SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-5] due to args.save_total_limit
  0%|▋                                                                                                                                                                                                                            | 15/5160 [00:21<1:39:29,  1.16s/it][INFO|trainer.py:4226] 2025-01-21 09:19:20,989 >>
{'loss': 1.7277, 'grad_norm': 107.13146209716797, 'learning_rate': 2.9953488372093026e-05, 'loss_1': 1.7165273427963257, 'loss_2': 0.011199951171875, 'loss_3': -15.168610572814941, 'loss_4': 10.847959518432617, 'epoch': 0.06}
{'loss': 1.8977, 'grad_norm': 117.21188354492188, 'learning_rate': 2.994767441860465e-05, 'loss_1': 1.8952716588974, 'loss_2': 0.002422332763671875, 'loss_3': -15.011028289794922, 'loss_4': 11.227081298828125, 'epoch': 0.07}
{'loss': 1.5942, 'grad_norm': 104.21891021728516, 'learning_rate': 2.994186046511628e-05, 'loss_1': 1.5918916463851929, 'loss_2': 0.002300262451171875, 'loss_3': -15.183256149291992, 'loss_4': 11.395156860351562, 'epoch': 0.08}
{'loss': 1.6214, 'grad_norm': 104.8545913696289, 'learning_rate': 2.9936046511627906e-05, 'loss_1': 1.6106793880462646, 'loss_2': 0.0107269287109375, 'loss_3': -15.328390121459961, 'loss_4': 10.86589527130127, 'epoch': 0.08}
{'loss': 1.3001, 'grad_norm': 113.33212280273438, 'learning_rate': 2.9930232558139534e-05, 'loss_1': 1.2887680530548096, 'loss_2': 0.01128387451171875, 'loss_3': -15.375351905822754, 'loss_4': 11.530424118041992, 'epoch': 0.09}
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-01-21 09:19:20,990 >>   Num examples = 1024
[INFO|trainer.py:4231] 2025-01-21 09:19:20,990 >>   Batch size = 64
  0%|▋                                                                                                                                                                                                                            | 15/5160 [00:25<1:39:29,  1.16s/it][INFO|trainer.py:3910] 2025-01-21 09:19:24,786 >> Saving model checkpoint to SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-15
[INFO|configuration_utils.py:420] 2025-01-21 09:19:24,788 >> Configuration saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-15/config.json                                                                               
{'eval_loss': 0.45326703786849976, 'eval_runtime': 3.795, 'eval_samples_per_second': 269.831, 'eval_steps_per_second': 4.216, 'eval_loss_1': 0.44685524702072144, 'eval_loss_2': 0.00641179084777832, 'eval_loss_3': -18.244651794433594, 'eval_loss_4': 11.200919151306152, 'epoch': 0.09}
[INFO|modeling_utils.py:2988] 2025-01-21 09:19:25,245 >> Model weights saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-15/model.safetensors
[INFO|tokenization_utils_base.py:2491] 2025-01-21 09:19:25,247 >> tokenizer config file saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-15/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-01-21 09:19:25,247 >> Special tokens file saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-15/special_tokens_map.json
[INFO|trainer.py:4002] 2025-01-21 09:19:26,158 >> Deleting older checkpoint [SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-10] due to args.save_total_limit
  0%|▊                                                                                                                                                                                                                            | 20/5160 [00:30<1:39:47,  1.16s/it][INFO|trainer.py:4226] 2025-01-21 09:19:29,827 >>
{'loss': 1.2278, 'grad_norm': 106.27903747558594, 'learning_rate': 2.9924418604651166e-05, 'loss_1': 1.2270082235336304, 'loss_2': 0.0007867813110351562, 'loss_3': -15.363876342773438, 'loss_4': 11.14828109741211, 'epoch': 0.09}
{'loss': 1.1616, 'grad_norm': 115.47207641601562, 'learning_rate': 2.991860465116279e-05, 'loss_1': 1.15428626537323, 'loss_2': 0.007293701171875, 'loss_3': -15.240442276000977, 'loss_4': 11.752845764160156, 'epoch': 0.1}
{'loss': 1.1334, 'grad_norm': 105.85124206542969, 'learning_rate': 2.991279069767442e-05, 'loss_1': 1.121359944343567, 'loss_2': 0.01204681396484375, 'loss_3': -15.175834655761719, 'loss_4': 11.774338722229004, 'epoch': 0.1}
{'loss': 1.018, 'grad_norm': 133.79824829101562, 'learning_rate': 2.9906976744186045e-05, 'loss_1': 1.013023018836975, 'loss_2': 0.00493621826171875, 'loss_3': -15.243135452270508, 'loss_4': 10.741376876831055, 'epoch': 0.11}
{'loss': 0.8247, 'grad_norm': 96.63512420654297, 'learning_rate': 2.9901162790697674e-05, 'loss_1': 0.8099250793457031, 'loss_2': 0.014739990234375, 'loss_3': -15.61720085144043, 'loss_4': 11.466066360473633, 'epoch': 0.12}
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-01-21 09:19:29,827 >>   Num examples = 1024
[INFO|trainer.py:4231] 2025-01-21 09:19:29,827 >>   Batch size = 64
  0%|▊                                                                                                                                                                                                                            | 20/5160 [00:34<1:39:47,  1.16s/it][INFO|trainer.py:3910] 2025-01-21 09:19:33,606 >> Saving model checkpoint to SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-20
[INFO|configuration_utils.py:420] 2025-01-21 09:19:33,608 >> Configuration saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-20/config.json                                                                               
{'eval_loss': 0.2591238021850586, 'eval_runtime': 3.7774, 'eval_samples_per_second': 271.085, 'eval_steps_per_second': 4.236, 'eval_loss_1': 0.23877665400505066, 'eval_loss_2': 0.020347118377685547, 'eval_loss_3': -18.226463317871094, 'eval_loss_4': 12.475502014160156, 'epoch': 0.12}
[INFO|modeling_utils.py:2988] 2025-01-21 09:19:34,115 >> Model weights saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-20/model.safetensors
[INFO|tokenization_utils_base.py:2491] 2025-01-21 09:19:34,116 >> tokenizer config file saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-20/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-01-21 09:19:34,116 >> Special tokens file saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-20/special_tokens_map.json
[INFO|trainer.py:4002] 2025-01-21 09:19:35,097 >> Deleting older checkpoint [SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-15] due to args.save_total_limit
  0%|█                                                                                                                                                                                                                            | 25/5160 [00:39<1:40:09,  1.17s/it][INFO|trainer.py:4226] 2025-01-21 09:19:38,725 >>
{'loss': 1.0587, 'grad_norm': 101.76844787597656, 'learning_rate': 2.9895348837209303e-05, 'loss_1': 1.0264846086502075, 'loss_2': 0.032196044921875, 'loss_3': -15.317649841308594, 'loss_4': 12.337306022644043, 'epoch': 0.12}
{'loss': 1.0104, 'grad_norm': 98.90593719482422, 'learning_rate': 2.988953488372093e-05, 'loss_1': 0.9755541682243347, 'loss_2': 0.03485107421875, 'loss_3': -15.630040168762207, 'loss_4': 12.728132247924805, 'epoch': 0.13}
{'loss': 0.7613, 'grad_norm': 83.88455963134766, 'learning_rate': 2.988372093023256e-05, 'loss_1': 0.7293928861618042, 'loss_2': 0.031951904296875, 'loss_3': -15.630891799926758, 'loss_4': 12.345390319824219, 'epoch': 0.13}
{'loss': 0.9171, 'grad_norm': 100.94139099121094, 'learning_rate': 2.9877906976744185e-05, 'loss_1': 0.889916718006134, 'loss_2': 0.0272064208984375, 'loss_3': -15.527692794799805, 'loss_4': 12.667590141296387, 'epoch': 0.14}
{'loss': 0.7596, 'grad_norm': 90.22350311279297, 'learning_rate': 2.9872093023255814e-05, 'loss_1': 0.7393615245819092, 'loss_2': 0.0202484130859375, 'loss_3': -15.379119873046875, 'loss_4': 12.293418884277344, 'epoch': 0.15}
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-01-21 09:19:38,725 >>   Num examples = 1024
[INFO|trainer.py:4231] 2025-01-21 09:19:38,725 >>   Batch size = 64
  0%|█                                                                                                                                                                                                                            | 25/5160 [00:43<1:40:09,  1.17s/it][INFO|trainer.py:3910] 2025-01-21 09:19:42,501 >> Saving model checkpoint to SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-25
[INFO|configuration_utils.py:420] 2025-01-21 09:19:42,503 >> Configuration saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-25/config.json                                                                               
{'eval_loss': 0.1916535347700119, 'eval_runtime': 3.7752, 'eval_samples_per_second': 271.242, 'eval_steps_per_second': 4.238, 'eval_loss_1': 0.18030884861946106, 'eval_loss_2': 0.011344671249389648, 'eval_loss_3': -18.187856674194336, 'eval_loss_4': 12.68618106842041, 'epoch': 0.15}
[INFO|modeling_utils.py:2988] 2025-01-21 09:19:43,002 >> Model weights saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-25/model.safetensors
[INFO|tokenization_utils_base.py:2491] 2025-01-21 09:19:43,004 >> tokenizer config file saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-25/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-01-21 09:19:43,004 >> Special tokens file saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-25/special_tokens_map.json
[INFO|trainer.py:4002] 2025-01-21 09:19:43,925 >> Deleting older checkpoint [SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-20] due to args.save_total_limit
  1%|█▎                                                                                                                                                                                                                           | 30/5160 [00:48<1:39:51,  1.17s/it][INFO|trainer.py:4226] 2025-01-21 09:19:47,571 >>
{'loss': 0.7218, 'grad_norm': 90.79553985595703, 'learning_rate': 2.9866279069767442e-05, 'loss_1': 0.7052597999572754, 'loss_2': 0.0164947509765625, 'loss_3': -15.2892427444458, 'loss_4': 11.83208179473877, 'epoch': 0.15}
{'loss': 0.7653, 'grad_norm': 86.20376586914062, 'learning_rate': 2.986046511627907e-05, 'loss_1': 0.7553814053535461, 'loss_2': 0.0099639892578125, 'loss_3': -15.240821838378906, 'loss_4': 12.321563720703125, 'epoch': 0.16}
{'loss': 0.5534, 'grad_norm': 83.89041137695312, 'learning_rate': 2.98546511627907e-05, 'loss_1': 0.5445038676261902, 'loss_2': 0.00890350341796875, 'loss_3': -15.134317398071289, 'loss_4': 11.762310981750488, 'epoch': 0.16}
{'loss': 0.6593, 'grad_norm': 88.47486877441406, 'learning_rate': 2.9848837209302325e-05, 'loss_1': 0.6507310271263123, 'loss_2': 0.00861358642578125, 'loss_3': -15.260749816894531, 'loss_4': 11.493511199951172, 'epoch': 0.17}
{'loss': 0.4542, 'grad_norm': 67.45013427734375, 'learning_rate': 2.9843023255813954e-05, 'loss_1': 0.44447579979896545, 'loss_2': 0.00977325439453125, 'loss_3': -15.121789932250977, 'loss_4': 11.905664443969727, 'epoch': 0.17}
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-01-21 09:19:47,572 >>   Num examples = 1024
[INFO|trainer.py:4231] 2025-01-21 09:19:47,572 >>   Batch size = 64
  1%|█▎                                                                                                                                                                                                                           | 30/5160 [00:52<1:39:51,  1.17s/it][INFO|trainer.py:3910] 2025-01-21 09:19:51,377 >> Saving model checkpoint to SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-30
[INFO|configuration_utils.py:420] 2025-01-21 09:19:51,379 >> Configuration saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-30/config.json                                                                               
{'eval_loss': 0.12845517694950104, 'eval_runtime': 3.8046, 'eval_samples_per_second': 269.147, 'eval_steps_per_second': 4.205, 'eval_loss_1': 0.12071661651134491, 'eval_loss_2': 0.007738560438156128, 'eval_loss_3': -18.18334197998047, 'eval_loss_4': 12.089934349060059, 'epoch': 0.17}
[INFO|modeling_utils.py:2988] 2025-01-21 09:19:51,904 >> Model weights saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-30/model.safetensors
[INFO|tokenization_utils_base.py:2491] 2025-01-21 09:19:51,905 >> tokenizer config file saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-30/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-01-21 09:19:51,906 >> Special tokens file saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-30/special_tokens_map.json
[INFO|trainer.py:4002] 2025-01-21 09:19:52,813 >> Deleting older checkpoint [SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-25] due to args.save_total_limit
  1%|█▍                                                                                                                                                                                                                           | 35/5160 [00:57<1:40:00,  1.17s/it][INFO|trainer.py:4226] 2025-01-21 09:19:56,469 >>
{'loss': 0.5389, 'grad_norm': 81.67945861816406, 'learning_rate': 2.9837209302325582e-05, 'loss_1': 0.5323687195777893, 'loss_2': 0.006500244140625, 'loss_3': -15.036050796508789, 'loss_4': 11.964696884155273, 'epoch': 0.18}
{'loss': 0.2796, 'grad_norm': 49.86838912963867, 'learning_rate': 2.983139534883721e-05, 'loss_1': 0.27905842661857605, 'loss_2': 0.0005173683166503906, 'loss_3': -15.127378463745117, 'loss_4': 10.730353355407715, 'epoch': 0.19}
{'loss': 0.3122, 'grad_norm': 56.6830940246582, 'learning_rate': 2.9825581395348836e-05, 'loss_1': 0.30902615189552307, 'loss_2': 0.00316619873046875, 'loss_3': -15.041460037231445, 'loss_4': 10.799007415771484, 'epoch': 0.19}
{'loss': 0.208, 'grad_norm': 41.09019470214844, 'learning_rate': 2.9819767441860465e-05, 'loss_1': 0.20172950625419617, 'loss_2': 0.0062408447265625, 'loss_3': -14.984981536865234, 'loss_4': 10.072652816772461, 'epoch': 0.2}
{'loss': 0.3267, 'grad_norm': 54.841880798339844, 'learning_rate': 2.9813953488372093e-05, 'loss_1': 0.3230195939540863, 'loss_2': 0.00372314453125, 'loss_3': -14.910869598388672, 'loss_4': 10.644375801086426, 'epoch': 0.2}
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-01-21 09:19:56,469 >>   Num examples = 1024
[INFO|trainer.py:4231] 2025-01-21 09:19:56,469 >>   Batch size = 64
  1%|█▍                                                                                                                                                                                                                           | 35/5160 [01:01<1:40:00,  1.17s/it][INFO|trainer.py:3910] 2025-01-21 09:20:00,273 >> Saving model checkpoint to SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-35
[INFO|configuration_utils.py:420] 2025-01-21 09:20:00,275 >> Configuration saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-35/config.json                                                                               
{'eval_loss': 0.06547199189662933, 'eval_runtime': 3.8026, 'eval_samples_per_second': 269.291, 'eval_steps_per_second': 4.208, 'eval_loss_1': 0.06086985394358635, 'eval_loss_2': 0.0046021342277526855, 'eval_loss_3': -18.221420288085938, 'eval_loss_4': 10.685587882995605, 'epoch': 0.2}
[INFO|modeling_utils.py:2988] 2025-01-21 09:20:00,770 >> Model weights saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-35/model.safetensors
[INFO|tokenization_utils_base.py:2491] 2025-01-21 09:20:00,771 >> tokenizer config file saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-35/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-01-21 09:20:00,771 >> Special tokens file saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-35/special_tokens_map.json
[INFO|trainer.py:4002] 2025-01-21 09:20:01,674 >> Deleting older checkpoint [SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-30] due to args.save_total_limit
  1%|█▋                                                                                                                                                                                                                           | 40/5160 [01:06<1:42:47,  1.20s/it][INFO|trainer.py:4226] 2025-01-21 09:20:05,488 >>
{'loss': 0.4213, 'grad_norm': 71.934814453125, 'learning_rate': 2.9808139534883722e-05, 'loss_1': 0.414447546005249, 'loss_2': 0.006885528564453125, 'loss_3': -15.198959350585938, 'loss_4': 10.67933464050293, 'epoch': 0.21}
{'loss': 0.2727, 'grad_norm': 55.80950927734375, 'learning_rate': 2.980232558139535e-05, 'loss_1': 0.27144855260849, 'loss_2': 0.0012226104736328125, 'loss_3': -15.08946704864502, 'loss_4': 10.605348587036133, 'epoch': 0.22}
{'loss': 0.3145, 'grad_norm': 62.07155227661133, 'learning_rate': 2.9796511627906976e-05, 'loss_1': 0.31256720423698425, 'loss_2': 0.0019350051879882812, 'loss_3': -14.903627395629883, 'loss_4': 10.632530212402344, 'epoch': 0.22}
{'loss': 0.2334, 'grad_norm': 46.162837982177734, 'learning_rate': 2.9790697674418604e-05, 'loss_1': 0.2262009084224701, 'loss_2': 0.0072174072265625, 'loss_3': -15.300891876220703, 'loss_4': 9.53083610534668, 'epoch': 0.23}
{'loss': 0.2368, 'grad_norm': 44.23947525024414, 'learning_rate': 2.9784883720930236e-05, 'loss_1': 0.23014727234840393, 'loss_2': 0.00665283203125, 'loss_3': -15.134639739990234, 'loss_4': 9.825296401977539, 'epoch': 0.23}
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-01-21 09:20:05,488 >>   Num examples = 1024
[INFO|trainer.py:4231] 2025-01-21 09:20:05,488 >>   Batch size = 64
  1%|█▋                                                                                                                                                                                                                           | 40/5160 [01:10<1:42:47,  1.20s/it][INFO|trainer.py:3910] 2025-01-21 09:20:09,507 >> Saving model checkpoint to SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-40
[INFO|configuration_utils.py:420] 2025-01-21 09:20:09,508 >> Configuration saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-40/config.json                                                                               
{'eval_loss': 0.0507834292948246, 'eval_runtime': 4.0174, 'eval_samples_per_second': 254.891, 'eval_steps_per_second': 3.983, 'eval_loss_1': 0.04430339112877846, 'eval_loss_2': 0.006480038166046143, 'eval_loss_3': -18.25812339782715, 'eval_loss_4': 9.579804420471191, 'epoch': 0.23}
[INFO|modeling_utils.py:2988] 2025-01-21 09:20:10,022 >> Model weights saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-40/model.safetensors
[INFO|tokenization_utils_base.py:2491] 2025-01-21 09:20:10,023 >> tokenizer config file saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-40/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-01-21 09:20:10,023 >> Special tokens file saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-40/special_tokens_map.json
[INFO|trainer.py:4002] 2025-01-21 09:20:10,932 >> Deleting older checkpoint [SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-35] due to args.save_total_limit
  1%|█▉                                                                                                                                                                                                                           | 45/5160 [01:15<1:41:33,  1.19s/it][INFO|trainer.py:4226] 2025-01-21 09:20:14,578 >>
{'loss': 0.2001, 'grad_norm': 34.41940689086914, 'learning_rate': 2.977906976744186e-05, 'loss_1': 0.18551290035247803, 'loss_2': 0.01458740234375, 'loss_3': -15.294574737548828, 'loss_4': 10.009132385253906, 'epoch': 0.24}
{'loss': 0.1824, 'grad_norm': 35.7876091003418, 'learning_rate': 2.977325581395349e-05, 'loss_1': 0.18143780529499054, 'loss_2': 0.0010042190551757812, 'loss_3': -15.248780250549316, 'loss_4': 8.763410568237305, 'epoch': 0.24}
{'loss': 0.1927, 'grad_norm': 44.27900695800781, 'learning_rate': 2.9767441860465116e-05, 'loss_1': 0.18381071090698242, 'loss_2': 0.0089111328125, 'loss_3': -15.34815502166748, 'loss_4': 9.029317855834961, 'epoch': 0.25}
{'loss': 0.2965, 'grad_norm': 30.05088233947754, 'learning_rate': 2.9761627906976744e-05, 'loss_1': 0.2886897921562195, 'loss_2': 0.0078582763671875, 'loss_3': -14.826355934143066, 'loss_4': 8.545624732971191, 'epoch': 0.26}
{'loss': 0.1768, 'grad_norm': 35.42728042602539, 'learning_rate': 2.9755813953488373e-05, 'loss_1': 0.1728733628988266, 'loss_2': 0.003936767578125, 'loss_3': -14.890892028808594, 'loss_4': 7.349732398986816, 'epoch': 0.26}
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-01-21 09:20:14,578 >>   Num examples = 1024
[INFO|trainer.py:4231] 2025-01-21 09:20:14,579 >>   Batch size = 64
  1%|█▉                                                                                                                                                                                                                           | 45/5160 [01:19<1:41:33,  1.19s/it][INFO|trainer.py:3910] 2025-01-21 09:20:18,385 >> Saving model checkpoint to SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-45
[INFO|configuration_utils.py:420] 2025-01-21 09:20:18,386 >> Configuration saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-45/config.json                                                                               
{'eval_loss': 0.0350908488035202, 'eval_runtime': 3.8052, 'eval_samples_per_second': 269.106, 'eval_steps_per_second': 4.205, 'eval_loss_1': 0.03125397488474846, 'eval_loss_2': 0.0038368701934814453, 'eval_loss_3': -18.1545467376709, 'eval_loss_4': 7.092556953430176, 'epoch': 0.26}
[INFO|modeling_utils.py:2988] 2025-01-21 09:20:18,881 >> Model weights saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-45/model.safetensors
[INFO|tokenization_utils_base.py:2491] 2025-01-21 09:20:18,883 >> tokenizer config file saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-45/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-01-21 09:20:18,883 >> Special tokens file saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-45/special_tokens_map.json
[INFO|trainer.py:4002] 2025-01-21 09:20:19,771 >> Deleting older checkpoint [SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-40] due to args.save_total_limit
  1%|██▏                                                                                                                                                                                                                          | 50/5160 [01:24<1:39:54,  1.17s/it][INFO|trainer.py:4226] 2025-01-21 09:20:23,422 >>
{'loss': 0.1166, 'grad_norm': 24.661739349365234, 'learning_rate': 2.975e-05, 'loss_1': 0.11499880254268646, 'loss_2': 0.0016145706176757812, 'loss_3': -15.032750129699707, 'loss_4': 7.006259441375732, 'epoch': 0.27}
{'loss': 0.0974, 'grad_norm': 20.708534240722656, 'learning_rate': 2.974418604651163e-05, 'loss_1': 0.09230950474739075, 'loss_2': 0.0050811767578125, 'loss_3': -15.103791236877441, 'loss_4': 5.713300704956055, 'epoch': 0.27}
{'loss': 0.1568, 'grad_norm': 40.773311614990234, 'learning_rate': 2.9738372093023255e-05, 'loss_1': 0.15644307434558868, 'loss_2': 0.0004031658172607422, 'loss_3': -14.872562408447266, 'loss_4': 5.514193534851074, 'epoch': 0.28}
{'loss': 0.1732, 'grad_norm': 44.278228759765625, 'learning_rate': 2.9732558139534884e-05, 'loss_1': 0.16572701930999756, 'loss_2': 0.00748443603515625, 'loss_3': -14.88737678527832, 'loss_4': 5.779683589935303, 'epoch': 0.28}
{'loss': 0.1199, 'grad_norm': 26.555389404296875, 'learning_rate': 2.9726744186046513e-05, 'loss_1': 0.11635076254606247, 'loss_2': 0.003505706787109375, 'loss_3': -14.900432586669922, 'loss_4': 4.913517951965332, 'epoch': 0.29}
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-01-21 09:20:23,422 >>   Num examples = 1024
[INFO|trainer.py:4231] 2025-01-21 09:20:23,422 >>   Batch size = 64
  1%|██▏                                                                                                                                                                                                                          | 50/5160 [01:28<1:39:54,  1.17s/it][INFO|trainer.py:3910] 2025-01-21 09:20:27,293 >> Saving model checkpoint to SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-50
[INFO|configuration_utils.py:420] 2025-01-21 09:20:27,295 >> Configuration saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-50/config.json                                                                               
{'eval_loss': 0.03160613030195236, 'eval_runtime': 3.8697, 'eval_samples_per_second': 264.623, 'eval_steps_per_second': 4.135, 'eval_loss_1': 0.0282704159617424, 'eval_loss_2': 0.003335714340209961, 'eval_loss_3': -18.00933265686035, 'eval_loss_4': 5.467982769012451, 'epoch': 0.29}
[INFO|modeling_utils.py:2988] 2025-01-21 09:20:27,844 >> Model weights saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-50/model.safetensors
[INFO|tokenization_utils_base.py:2491] 2025-01-21 09:20:27,846 >> tokenizer config file saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-50/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-01-21 09:20:27,846 >> Special tokens file saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-50/special_tokens_map.json
[INFO|trainer.py:4002] 2025-01-21 09:20:28,792 >> Deleting older checkpoint [SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-45] due to args.save_total_limit
  1%|██▎                                                                                                                                                                                                                          | 55/5160 [01:33<1:40:37,  1.18s/it][INFO|trainer.py:4226] 2025-01-21 09:20:32,454 >>
{'loss': 0.1377, 'grad_norm': 28.872817993164062, 'learning_rate': 2.972093023255814e-05, 'loss_1': 0.1308414340019226, 'loss_2': 0.00690460205078125, 'loss_3': -14.75455093383789, 'loss_4': 5.3029279708862305, 'epoch': 0.3}
{'loss': 0.118, 'grad_norm': 28.40202522277832, 'learning_rate': 2.971511627906977e-05, 'loss_1': 0.1077355220913887, 'loss_2': 0.0102996826171875, 'loss_3': -14.74593734741211, 'loss_4': 5.680530548095703, 'epoch': 0.3}
{'loss': 0.0666, 'grad_norm': 18.48625373840332, 'learning_rate': 2.9709302325581395e-05, 'loss_1': 0.06252453476190567, 'loss_2': 0.004085540771484375, 'loss_3': -14.960039138793945, 'loss_4': 4.7822113037109375, 'epoch': 0.31}
{'loss': 0.1094, 'grad_norm': 28.776592254638672, 'learning_rate': 2.9703488372093024e-05, 'loss_1': 0.1060275286436081, 'loss_2': 0.0033416748046875, 'loss_3': -14.636248588562012, 'loss_4': 4.737236022949219, 'epoch': 0.31}
{'loss': 0.1181, 'grad_norm': 30.663402557373047, 'learning_rate': 2.9697674418604652e-05, 'loss_1': 0.11469127982854843, 'loss_2': 0.00341796875, 'loss_3': -14.916358947753906, 'loss_4': 4.0490946769714355, 'epoch': 0.32}
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-01-21 09:20:32,454 >>   Num examples = 1024
[INFO|trainer.py:4231] 2025-01-21 09:20:32,454 >>   Batch size = 64
  1%|██▌                                                                                                                                                                                                                          | 60/5160 [01:40<1:31:11,  1.07s/it][INFO|trainer.py:4226] 2025-01-21 09:20:39,911 >>
***** Running Evaluation *****                                                                                                                                                                                                                                        
{'eval_loss': 0.044894423335790634, 'eval_runtime': 3.8798, 'eval_samples_per_second': 263.929, 'eval_steps_per_second': 4.124, 'eval_loss_1': 0.03911501169204712, 'eval_loss_2': 0.0057794153690338135, 'eval_loss_3': -17.943927764892578, 'eval_loss_4': 4.743851661682129, 'epoch': 0.32}
{'loss': 0.0883, 'grad_norm': 20.38454818725586, 'learning_rate': 2.969186046511628e-05, 'loss_1': 0.08065195381641388, 'loss_2': 0.00762176513671875, 'loss_3': -14.94198226928711, 'loss_4': 4.497425079345703, 'epoch': 0.33}
{'loss': 0.1108, 'grad_norm': 25.677841186523438, 'learning_rate': 2.9686046511627906e-05, 'loss_1': 0.1075904443860054, 'loss_2': 0.003215789794921875, 'loss_3': -14.923700332641602, 'loss_4': 4.784696578979492, 'epoch': 0.33}
{'loss': 0.1714, 'grad_norm': 37.216312408447266, 'learning_rate': 2.9680232558139535e-05, 'loss_1': 0.16724742949008942, 'loss_2': 0.00411224365234375, 'loss_3': -14.758265495300293, 'loss_4': 4.421056747436523, 'epoch': 0.34}
{'loss': 0.1078, 'grad_norm': 31.590885162353516, 'learning_rate': 2.9674418604651164e-05, 'loss_1': 0.10507144778966904, 'loss_2': 0.0027217864990234375, 'loss_3': -14.83881664276123, 'loss_4': 4.454354763031006, 'epoch': 0.34}
{'loss': 0.1043, 'grad_norm': 22.441402435302734, 'learning_rate': 2.9668604651162792e-05, 'loss_1': 0.10117213428020477, 'loss_2': 0.003170013427734375, 'loss_3': -14.979935646057129, 'loss_4': 3.934015989303589, 'epoch': 0.35}
[INFO|trainer.py:4228] 2025-01-21 09:20:39,911 >>   Num examples = 1024
[INFO|trainer.py:4231] 2025-01-21 09:20:39,912 >>   Batch size = 64
  1%|██▌                                                                                                                                                                                                                          | 60/5160 [01:44<1:31:11,  1.07s/it][INFO|trainer.py:3910] 2025-01-21 09:20:43,741 >> Saving model checkpoint to SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-60
[INFO|configuration_utils.py:420] 2025-01-21 09:20:43,744 >> Configuration saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-60/config.json                                                                               
{'eval_loss': 0.025551997125148773, 'eval_runtime': 3.8278, 'eval_samples_per_second': 267.515, 'eval_steps_per_second': 4.18, 'eval_loss_1': 0.022219887003302574, 'eval_loss_2': 0.00333210825920105, 'eval_loss_3': -18.05217170715332, 'eval_loss_4': 5.016391277313232, 'epoch': 0.35}
[INFO|modeling_utils.py:2988] 2025-01-21 09:20:44,285 >> Model weights saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-60/model.safetensors
[INFO|tokenization_utils_base.py:2491] 2025-01-21 09:20:44,287 >> tokenizer config file saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-60/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-01-21 09:20:44,287 >> Special tokens file saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-60/special_tokens_map.json
[INFO|trainer.py:4002] 2025-01-21 09:20:45,251 >> Deleting older checkpoint [SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-50] due to args.save_total_limit
  1%|██▊                                                                                                                                                                                                                          | 65/5160 [01:49<1:39:12,  1.17s/it][INFO|trainer.py:4226] 2025-01-21 09:20:48,958 >>
{'loss': 0.1319, 'grad_norm': 36.32752990722656, 'learning_rate': 2.966279069767442e-05, 'loss_1': 0.12761501967906952, 'loss_2': 0.00428009033203125, 'loss_3': -14.774948120117188, 'loss_4': 4.542036056518555, 'epoch': 0.35}
{'loss': 0.0949, 'grad_norm': 23.38616943359375, 'learning_rate': 2.9656976744186046e-05, 'loss_1': 0.09353280067443848, 'loss_2': 0.0013742446899414062, 'loss_3': -15.018609046936035, 'loss_4': 4.940299987792969, 'epoch': 0.36}
{'loss': 0.1195, 'grad_norm': 31.76519203186035, 'learning_rate': 2.9651162790697675e-05, 'loss_1': 0.116350457072258, 'loss_2': 0.003139495849609375, 'loss_3': -14.886391639709473, 'loss_4': 5.206482887268066, 'epoch': 0.37}
{'loss': 0.0878, 'grad_norm': 26.03571319580078, 'learning_rate': 2.9645348837209303e-05, 'loss_1': 0.0840296596288681, 'loss_2': 0.003803253173828125, 'loss_3': -15.10847282409668, 'loss_4': 5.949328422546387, 'epoch': 0.37}
{'loss': 0.072, 'grad_norm': 19.741825103759766, 'learning_rate': 2.9639534883720932e-05, 'loss_1': 0.06629898399114609, 'loss_2': 0.00571441650390625, 'loss_3': -15.112955093383789, 'loss_4': 6.810018539428711, 'epoch': 0.38}
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-01-21 09:20:48,959 >>   Num examples = 1024
[INFO|trainer.py:4231] 2025-01-21 09:20:48,959 >>   Batch size = 64
  1%|██▊                                                                                                                                                                                                                          | 65/5160 [01:53<1:39:12,  1.17s/it][INFO|trainer.py:3910] 2025-01-21 09:20:52,777 >> Saving model checkpoint to SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-65
[INFO|configuration_utils.py:420] 2025-01-21 09:20:52,780 >> Configuration saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-65/config.json                                                                               
{'eval_loss': 0.022795291617512703, 'eval_runtime': 3.8177, 'eval_samples_per_second': 268.223, 'eval_steps_per_second': 4.191, 'eval_loss_1': 0.01712566800415516, 'eval_loss_2': 0.005669623613357544, 'eval_loss_3': -18.193586349487305, 'eval_loss_4': 7.123434543609619, 'epoch': 0.38}
[INFO|modeling_utils.py:2988] 2025-01-21 09:20:53,271 >> Model weights saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-65/model.safetensors
[INFO|tokenization_utils_base.py:2491] 2025-01-21 09:20:53,273 >> tokenizer config file saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-65/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-01-21 09:20:53,273 >> Special tokens file saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/checkpoint-65/special_tokens_map.json
Traceback (most recent call last):
  File "/mnt/data2/lian/projects/watermark/watermark-simcse/SimCSE/train.py", line 656, in <module>
    main()
  File "/mnt/data2/lian/projects/watermark/watermark-simcse/SimCSE/train.py", line 620, in main
    train_result = trainer.train(model_path=model_path)
  File "/mnt/data/lian/anaconda3/envs/watermark-simcse/lib/python3.10/site-packages/transformers/trainer.py", line 2171, in train
    return inner_training_loop(
  File "/mnt/data/lian/anaconda3/envs/watermark-simcse/lib/python3.10/site-packages/transformers/trainer.py", line 2598, in _inner_training_loop
    self._maybe_log_save_evaluate(
  File "/mnt/data/lian/anaconda3/envs/watermark-simcse/lib/python3.10/site-packages/transformers/trainer.py", line 3078, in _maybe_log_save_evaluate
    self._save_checkpoint(model, trial)
  File "/mnt/data/lian/anaconda3/envs/watermark-simcse/lib/python3.10/site-packages/transformers/trainer.py", line 3213, in _save_checkpoint
    self._save_optimizer_and_scheduler(output_dir)
  File "/mnt/data/lian/anaconda3/envs/watermark-simcse/lib/python3.10/site-packages/transformers/trainer.py", line 3334, in _save_optimizer_and_scheduler
    torch.save(self.optimizer.state_dict(), os.path.join(output_dir, OPTIMIZER_NAME))
  File "/mnt/data/lian/anaconda3/envs/watermark-simcse/lib/python3.10/site-packages/torch/serialization.py", line 850, in save
    _save(
  File "/mnt/data/lian/anaconda3/envs/watermark-simcse/lib/python3.10/site-packages/torch/serialization.py", line 1114, in _save
    zip_file.write_record(name, storage, num_bytes)
KeyboardInterrupt
Traceback (most recent call last):
  File "/mnt/data2/lian/projects/watermark/watermark-simcse/SimCSE/train.py", line 656, in <module>
    main()
  File "/mnt/data2/lian/projects/watermark/watermark-simcse/SimCSE/train.py", line 620, in main
    train_result = trainer.train(model_path=model_path)
  File "/mnt/data/lian/anaconda3/envs/watermark-simcse/lib/python3.10/site-packages/transformers/trainer.py", line 2171, in train
    return inner_training_loop(
  File "/mnt/data/lian/anaconda3/envs/watermark-simcse/lib/python3.10/site-packages/transformers/trainer.py", line 2598, in _inner_training_loop
    self._maybe_log_save_evaluate(
  File "/mnt/data/lian/anaconda3/envs/watermark-simcse/lib/python3.10/site-packages/transformers/trainer.py", line 3078, in _maybe_log_save_evaluate
    self._save_checkpoint(model, trial)
  File "/mnt/data/lian/anaconda3/envs/watermark-simcse/lib/python3.10/site-packages/transformers/trainer.py", line 3213, in _save_checkpoint
    self._save_optimizer_and_scheduler(output_dir)
  File "/mnt/data/lian/anaconda3/envs/watermark-simcse/lib/python3.10/site-packages/transformers/trainer.py", line 3334, in _save_optimizer_and_scheduler
    torch.save(self.optimizer.state_dict(), os.path.join(output_dir, OPTIMIZER_NAME))
  File "/mnt/data/lian/anaconda3/envs/watermark-simcse/lib/python3.10/site-packages/torch/serialization.py", line 850, in save
    _save(
  File "/mnt/data/lian/anaconda3/envs/watermark-simcse/lib/python3.10/site-packages/torch/serialization.py", line 1114, in _save
    zip_file.write_record(name, storage, num_bytes)
KeyboardInterrupt
