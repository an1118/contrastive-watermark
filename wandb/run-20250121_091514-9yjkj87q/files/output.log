  0%|                                                                                                                                                              | 0/4 [00:00<?, ?it/s][WARNING|logging.py:313] 2025-01-21 09:15:14,758 >> You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  5.26it/s][INFO|trainer.py:2643] 2025-01-21 09:15:15,807 >>
{'loss': 0.8248, 'grad_norm': nan, 'learning_rate': 3e-05, 'loss_1': 0.7656834721565247, 'loss_2': 0.05908203125, 'loss_3': -13.56039810180664, 'loss_4': 8.684257507324219, 'epoch': 0.25}
{'loss': 1.01, 'grad_norm': nan, 'learning_rate': 3e-05, 'loss_1': 0.9266692399978638, 'loss_2': 0.08331298828125, 'loss_3': -14.017597198486328, 'loss_4': 9.819067001342773, 'epoch': 0.5}
{'loss': 1.2577, 'grad_norm': nan, 'learning_rate': 3e-05, 'loss_1': 1.1717592477798462, 'loss_2': 0.0859375, 'loss_3': -13.118681907653809, 'loss_4': 11.13354778289795, 'epoch': 0.75}
{'loss': 1.6179, 'grad_norm': nan, 'learning_rate': 3e-05, 'loss_1': 1.5405558347702026, 'loss_2': 0.07733154296875, 'loss_3': -14.324337005615234, 'loss_4': 11.054439544677734, 'epoch': 1.0}

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  3.79it/s]
{'train_runtime': 1.9597, 'train_samples_per_second': 16.329, 'train_steps_per_second': 2.041, 'train_loss': 1.1775829643011093, 'epoch': 1.0}
[INFO|trainer.py:3910] 2025-01-21 09:15:15,808 >> Saving model checkpoint to SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1
[INFO|configuration_utils.py:420] 2025-01-21 09:15:15,810 >> Configuration saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/config.json
[INFO|modeling_utils.py:2988] 2025-01-21 09:15:16,243 >> Model weights saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/model.safetensors
[INFO|tokenization_utils_base.py:2491] 2025-01-21 09:15:16,244 >> tokenizer config file saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/tokenizer_config.json
[INFO|tokenization_utils_base.py:2500] 2025-01-21 09:15:16,245 >> Special tokens file saved in SimCSE/result/twitter-roberta-base-sentiment/end2end-c4-loss_cl2_gr-wneg1/special_tokens_map.json
01/21/2025 09:15:16 - INFO - __main__ -   ***** Train results *****
01/21/2025 09:15:16 - INFO - __main__ -     epoch = 1.0
01/21/2025 09:15:16 - INFO - __main__ -     total_flos = 15992086855680.0
01/21/2025 09:15:16 - INFO - __main__ -     train_loss = 1.1775829643011093
01/21/2025 09:15:16 - INFO - __main__ -     train_runtime = 1.9597
01/21/2025 09:15:16 - INFO - __main__ -     train_samples_per_second = 16.329
01/21/2025 09:15:16 - INFO - __main__ -     train_steps_per_second = 2.041
01/21/2025 09:15:16 - INFO - __main__ -   *** Evaluate ***
[INFO|trainer.py:4226] 2025-01-21 09:15:16,283 >>
***** Running Evaluation *****
[INFO|trainer.py:4228] 2025-01-21 09:15:16,283 >>   Num examples = 8
[INFO|trainer.py:4231] 2025-01-21 09:15:16,283 >>   Batch size = 8
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1523.54it/s]
01/21/2025 09:15:16 - INFO - __main__ -   ***** Eval results *****
01/21/2025 09:15:16 - INFO - __main__ -     epoch = 1.0
01/21/2025 09:15:16 - INFO - __main__ -     eval_loss = 0.39591631293296814
01/21/2025 09:15:16 - INFO - __main__ -     eval_loss_1 = 0.31107744574546814
01/21/2025 09:15:16 - INFO - __main__ -     eval_loss_2 = 0.0848388671875
01/21/2025 09:15:16 - INFO - __main__ -     eval_loss_3 = -18.263883590698242
01/21/2025 09:15:16 - INFO - __main__ -     eval_loss_4 = 13.748615264892578
01/21/2025 09:15:16 - INFO - __main__ -     eval_runtime = 0.0378
01/21/2025 09:15:16 - INFO - __main__ -     eval_samples_per_second = 211.876
01/21/2025 09:15:16 - INFO - __main__ -     eval_steps_per_second = 26.485
