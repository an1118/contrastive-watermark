  0%|                                                                                                                                                              | 0/8 [00:00<?, ?it/s][WARNING|logging.py:313] 2025-01-21 07:00:16,978 >> You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 38%|████████████████████████████████████████████████████████▎                                                                                             | 3/8 [00:03<00:05,  1.16s/it]Traceback (most recent call last):
{'loss': 2.1509, 'grad_norm': nan, 'learning_rate': 3e-05, 'loss_1': 2.142784595489502, 'loss_2': 0.0081634521484375, 'loss_3': -9.328385353088379, 'loss_4': 8.89427375793457, 'epoch': 0.25}
{'loss': 2.3406, 'grad_norm': nan, 'learning_rate': 3e-05, 'loss_1': 2.2998580932617188, 'loss_2': 0.04071044921875, 'loss_3': -10.609344482421875, 'loss_4': 10.623645782470703, 'epoch': 0.5}
{'loss': 1.2036, 'grad_norm': nan, 'learning_rate': 3e-05, 'loss_1': 1.188690185546875, 'loss_2': 0.01488494873046875, 'loss_3': -11.12866497039795, 'loss_4': 10.601709365844727, 'epoch': 0.75}
  File "/mnt/data2/lian/projects/watermark/watermark-simcse/SimCSE-Watermark/train.py", line 656, in <module>
    main()
  File "/mnt/data2/lian/projects/watermark/watermark-simcse/SimCSE-Watermark/train.py", line 620, in main
    train_result = trainer.train(model_path=model_path)
  File "/mnt/data/lian/anaconda3/envs/watermark-simcse/lib/python3.10/site-packages/transformers/trainer.py", line 2171, in train
    return inner_training_loop(
  File "/mnt/data/lian/anaconda3/envs/watermark-simcse/lib/python3.10/site-packages/transformers/trainer.py", line 2531, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/mnt/data/lian/anaconda3/envs/watermark-simcse/lib/python3.10/site-packages/transformers/trainer.py", line 3675, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
  File "/mnt/data2/lian/projects/watermark/watermark-simcse/SimCSE-Watermark/simcse/trainers.py", line 585, in compute_loss
    outputs = model(**inputs)
  File "/mnt/data/lian/anaconda3/envs/watermark-simcse/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/data/lian/anaconda3/envs/watermark-simcse/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/lian/anaconda3/envs/watermark-simcse/lib/python3.10/site-packages/accelerate/utils/operations.py", line 820, in forward
    return model_forward(*args, **kwargs)
  File "/mnt/data/lian/anaconda3/envs/watermark-simcse/lib/python3.10/site-packages/accelerate/utils/operations.py", line 808, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/mnt/data/lian/anaconda3/envs/watermark-simcse/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
  File "/mnt/data2/lian/projects/watermark/watermark-simcse/SimCSE-Watermark/simcse/models.py", line 643, in forward
    return cl_forward(self,
  File "/mnt/data2/lian/projects/watermark/watermark-simcse/SimCSE-Watermark/simcse/models.py", line 256, in cl_forward
    pooler_output = last_token_pool(outputs.last_hidden_state, attention_mask)
  File "/mnt/data2/lian/projects/watermark/watermark-simcse/SimCSE-Watermark/simcse/models.py", line 237, in last_token_pool
    if left_padding:
KeyboardInterrupt
Traceback (most recent call last):
  File "/mnt/data2/lian/projects/watermark/watermark-simcse/SimCSE-Watermark/train.py", line 656, in <module>
    main()
  File "/mnt/data2/lian/projects/watermark/watermark-simcse/SimCSE-Watermark/train.py", line 620, in main
    train_result = trainer.train(model_path=model_path)
  File "/mnt/data/lian/anaconda3/envs/watermark-simcse/lib/python3.10/site-packages/transformers/trainer.py", line 2171, in train
    return inner_training_loop(
  File "/mnt/data/lian/anaconda3/envs/watermark-simcse/lib/python3.10/site-packages/transformers/trainer.py", line 2531, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/mnt/data/lian/anaconda3/envs/watermark-simcse/lib/python3.10/site-packages/transformers/trainer.py", line 3675, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
  File "/mnt/data2/lian/projects/watermark/watermark-simcse/SimCSE-Watermark/simcse/trainers.py", line 585, in compute_loss
    outputs = model(**inputs)
  File "/mnt/data/lian/anaconda3/envs/watermark-simcse/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/data/lian/anaconda3/envs/watermark-simcse/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data/lian/anaconda3/envs/watermark-simcse/lib/python3.10/site-packages/accelerate/utils/operations.py", line 820, in forward
    return model_forward(*args, **kwargs)
  File "/mnt/data/lian/anaconda3/envs/watermark-simcse/lib/python3.10/site-packages/accelerate/utils/operations.py", line 808, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/mnt/data/lian/anaconda3/envs/watermark-simcse/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
  File "/mnt/data2/lian/projects/watermark/watermark-simcse/SimCSE-Watermark/simcse/models.py", line 643, in forward
    return cl_forward(self,
  File "/mnt/data2/lian/projects/watermark/watermark-simcse/SimCSE-Watermark/simcse/models.py", line 256, in cl_forward
    pooler_output = last_token_pool(outputs.last_hidden_state, attention_mask)
  File "/mnt/data2/lian/projects/watermark/watermark-simcse/SimCSE-Watermark/simcse/models.py", line 237, in last_token_pool
    if left_padding:
KeyboardInterrupt
